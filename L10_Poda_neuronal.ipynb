{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP3wWXBHt1fBqV0UMWo9uN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Carlos-Villasenor/Clase_Aprendizaje_Profundo/blob/main/L10_Poda_neuronal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFv8P5j4oy_d"
      },
      "source": [
        "# Aprendizaje Profundo\n",
        "# Neural Pruning\n",
        "##Dr. Carlos Villaseñor\n",
        "\n",
        "(Ejemplo basado en [enlace](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD9QWY3wpTmv"
      },
      "source": [
        "Paso 1. Instalamos la biblioteca para"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "8qOYFCKGSIcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 2. Importamos los paquetes necesarios"
      ],
      "metadata": {
        "id": "9wnxu-VgRrnO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlOWGLD7oyGc"
      },
      "source": [
        "# Bibliotecas principales\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Paquetería de poda neuronal\n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenar el modelo base"
      ],
      "metadata": {
        "id": "F1b8ZiKmbgO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3. Importamos los datos de MNIST y normalizamos las imagenes"
      ],
      "metadata": {
        "id": "tEpTgpGqSu62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "ZWNgI3BxS4WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4. Creamos una red neuronal convolucional"
      ],
      "metadata": {
        "id": "WcynobInTEL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la arquitectura\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Compilamos el modelo\n",
        "model.compile(optimizer='adam',\n",
        "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "          metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yN-FaJBoTKC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 5. Entrenamos el modelo"
      ],
      "metadata": {
        "id": "aV1sw5HWTZLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, epochs=4, validation_split=0.1)"
      ],
      "metadata": {
        "id": "Zli2cYfHTc-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 6. Evaluamos el modelo"
      ],
      "metadata": {
        "id": "W6_kl6VZTjJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, base_model_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Original model :', base_model_acc)"
      ],
      "metadata": {
        "id": "7BIqJrmXToKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3993d2-28d2-4684-8e8c-ff227dec4bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model : 0.9753999710083008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 7. Guardamos el modelo en memoria secundaria"
      ],
      "metadata": {
        "id": "fUIgjMgpT1S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.models.save_model(model, 'base_model.h5', include_optimizer=False)"
      ],
      "metadata": {
        "id": "m-Co4VPmT4mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Poda neuronal"
      ],
      "metadata": {
        "id": "rcC_J9y0bldG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 8. Empecemos a usar la poda neuronal, definamos un modelo especial para hacer la poda. En el siguiente bloque prepararemos un modelo para hacer prunning sobre todo el modelo, también es posible hacer prunning en capas especificas como pueden verlo en este [enlace](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude)."
      ],
      "metadata": {
        "id": "oaHji_-wUmA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos objeto de poda para quitar\n",
        "Pruner = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "\n",
        "# Necesitamos calcular cuantos pasos daremos en el proceso de poda\n",
        "# para esto definimos los siguientes parámetros\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1\n",
        "\n",
        "# Calcular end-step\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Definimos los parámetros para el esquema de poda\n",
        "pruning_params = {\n",
        "'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                           final_sparsity=0.80,\n",
        "                                                           begin_step=0,\n",
        "                                                           end_step=end_step)\n",
        "}\n",
        "\n",
        "# Creamos el modelo para podar\n",
        "pruned_model = Pruner(model, **pruning_params)\n",
        "\n",
        "# Es necesario recompilar el modelo.\n",
        "pruned_model.compile(optimizer='adam',\n",
        "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "          metrics=['accuracy'])\n",
        "\n",
        "pruned_model.summary()"
      ],
      "metadata": {
        "id": "E-PTYUDLU7yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 9. El proceso de podado neuronal. Como cualquier otro modelo entrenemos el modelo original con un proceso de poda."
      ],
      "metadata": {
        "id": "R1f3by81WTYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un par de callback para usar la poda\n",
        "callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
        "\n",
        "# Pdamos nuestro modelo\n",
        "pruned_model.fit(train_images, train_labels,\n",
        "                  batch_size=batch_size, epochs=epochs,\n",
        "                  validation_split=validation_split,\n",
        "                  callbacks=callbacks)"
      ],
      "metadata": {
        "id": "c0F8MIUZXD3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 10. Comparemos ambos modelos."
      ],
      "metadata": {
        "id": "SihOY0QcXkiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, pruned_model_acc = pruned_model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Original test accuracy:', base_model_acc)\n",
        "print('Pruned test accuracy:', pruned_model_acc)"
      ],
      "metadata": {
        "id": "yIG6yQE0Xo4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 11. Comprimimos el modelo. Ahora que el modelo ha sido padado. Muchos de las neuronas no tienen contribución pero el modelo. es igual de grande; ahora vamos a comprimirlo para tener un modelo más pequeño en memoria."
      ],
      "metadata": {
        "id": "mZC0ha88Y2Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "\n",
        "print(compressed_model.summary())\n",
        "\n",
        "tf.keras.models.save_model(compressed_model, 'compressed_model.h5',\n",
        "                           include_optimizer=False)"
      ],
      "metadata": {
        "id": "-JCI0wWAZZHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow Lite"
      ],
      "metadata": {
        "id": "vPlmxS4cbVtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 12. Para hadware de bajo rendimiento es mejor usar una versión reducida de TensorFlow, llamada TensorFlow Lite. Este framework reducido permite correr modelos de Aprendizaje profundo en hadwares pequeños o embebidos."
      ],
      "metadata": {
        "id": "ZT3aIChUZ22E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(compressed_model)\n",
        "compressed_model_lite = converter.convert()\n",
        "\n",
        "with open('compressed_model_lite.tflite', 'wb') as f:\n",
        "  f.write(compressed_model_lite)"
      ],
      "metadata": {
        "id": "TvC37NnJZTid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Poda y cuantización"
      ],
      "metadata": {
        "id": "Gr88Lv4VbQuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 13. Usaremos la poda neuronal en conjunto con las técnica de cuantización para comprimir nuestro modelo aun más."
      ],
      "metadata": {
        "id": "j6PKlgCobNtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(compressed_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_and_pruned_lite_model = converter.convert()\n",
        "\n",
        "with open('quantized_and_pruned_model.tflite', 'wb') as f:\n",
        "  f.write(quantized_and_pruned_lite_model)"
      ],
      "metadata": {
        "id": "2955kQMUb8cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 14. Para poder comparar el modelo anterior, sirvase de la siguiente función que evaluara el modelo de TFLite interpretado dentro de nuestro entorno."
      ],
      "metadata": {
        "id": "forqqvxidr-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on ever y image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_lite_model)\n",
        "interpreter.allocate_tensors()\n",
        "test_accuracy = evaluate_model(interpreter)"
      ],
      "metadata": {
        "id": "pYKqtdLucxVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparación final"
      ],
      "metadata": {
        "id": "eZljkWWHeV9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 15. Finalmente vemos la comparación entre"
      ],
      "metadata": {
        "id": "ed9Z_NG7d4VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Modelo original:', base_model_acc)\n",
        "print('Modelo podado:', pruned_model_acc)\n",
        "print('Modelo podado y cuantizado para TFlite:', test_accuracy)"
      ],
      "metadata": {
        "id": "IMWi0z2ac3mt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}