{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM53R+FBM198TEsBP7HluOZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Carlos-Villasenor/Clase_Aprendizaje_Profundo/blob/main/L05_Red_Neuronal_Densa_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red Neuronal Densa en TensorFlow\n",
        "## Dr. Carlos Villaseñor"
      ],
      "metadata": {
        "id": "OA2bDa4ZR5qJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 1. Correr paqueterias"
      ],
      "metadata": {
        "id": "pXflGAH6SC_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "ckWvgWOJRp_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 2. Descarga el siguiente Data-set"
      ],
      "metadata": {
        "id": "EbjVjdXsbz7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = keras.utils.get_file('auto-mpg.data', 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data')"
      ],
      "metadata": {
        "id": "7zFo1FFHbr2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3. Limpieza de datos"
      ],
      "metadata": {
        "id": "WB4OaHFwb8tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\" \", skipinitialspace=True)\n",
        "\n",
        "dataset = raw_dataset.copy()\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "NZ4a1lS6b-_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4. Revisar datos faltantes"
      ],
      "metadata": {
        "id": "Sl08EJPhcDY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.isna().sum()"
      ],
      "metadata": {
        "id": "IBJOZ4TpcJxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 5. Quitar registros con datos faltantes"
      ],
      "metadata": {
        "id": "m6WGXqFJcNIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.dropna()"
      ],
      "metadata": {
        "id": "pFUDue1DcTsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 6. Condificación one-hot a columnas del país"
      ],
      "metadata": {
        "id": "yBTGc24LcWUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "origin = dataset.pop('Origin')\n",
        "dataset['USA'] = (origin == 1)*1.0\n",
        "dataset['Europe'] = (origin == 2)*1.0\n",
        "dataset['Japan'] = (origin == 3)*1.0\n",
        "dataset.tail()"
      ],
      "metadata": {
        "id": "mai3WIvjceos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 7. Elige las variables de entrada y salida"
      ],
      "metadata": {
        "id": "iTgPuiGRcj68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.asanyarray(dataset.drop(columns=['MPG']))\n",
        "y = np.asanyarray(dataset[['MPG']])\n",
        "x = StandardScaler().fit_transform(x)\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "VrIZydVsctDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 8. Particiona el dataset"
      ],
      "metadata": {
        "id": "XDS6Y3kzcvey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "qIzfIZA5c0Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 9. Crea una red neuronal para aprender los datos"
      ],
      "metadata": {
        "id": "aPeO7Sx3c43I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Dense(128, activation='relu', input_shape=[x.shape[1]]))\n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Dense(32, activation='tanh'))\n",
        "  model.add(keras.layers.Dense(1, activation='linear'))\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=keras.optimizers.Adam())\n",
        "  return  model"
      ],
      "metadata": {
        "id": "lyqOvKe6c-hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "6GjGhqSodG2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 10. Entrena la red neuronal"
      ],
      "metadata": {
        "id": "wP2b0dINdLrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(xtrain, ytrain,\n",
        "                    batch_size=150, epochs=3000,\n",
        "                    validation_data=(xtest, ytest), verbose=1)"
      ],
      "metadata": {
        "id": "mc64yAzwdQTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 11. Dibuja las curvas de aprendizaje para un mejor diagnóstico del aprendizaje"
      ],
      "metadata": {
        "id": "LqX0YjwueCBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  plt.figure()\n",
        "  plt.title('Loss vs Iterations')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(hist['epoch'], hist['loss'],\n",
        "           label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_loss'],\n",
        "           label = 'Val Error')\n",
        "  plt.ylim([0,300])\n",
        "  plt.legend()\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "qEPR2AkFeBNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 12. Implementación de Eary Stopping como regulador"
      ],
      "metadata": {
        "id": "YwlydF57eSPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=20)\n",
        "model = build_model()\n",
        "history = model.fit(xtrain, ytrain,\n",
        "                    batch_size=150, epochs=3000, \n",
        "                    validation_data=(xtest, ytest),\n",
        "                    verbose=1, callbacks=[es])\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "IePGJkn2eR2M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}