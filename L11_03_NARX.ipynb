{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRN7Jc5J6kR10dAsZ2Nl4H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Carlos-Villasenor/Clase_Aprendizaje_Profundo/blob/main/L11_03_NARX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFv8P5j4oy_d"
      },
      "source": [
        "# Deep Learning\n",
        "##Dr. Carlos Villaseñor\n",
        "## NARX(p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "N72BjpHOEIiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Activations functions\n",
        "Dr. Carlos Villaseñor\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Activation functions for output layer ------------------------------\n",
        "\n",
        "def linear(z, derivative=False):\n",
        "    a = z\n",
        "    if derivative:\n",
        "        da = np.ones(z.shape)\n",
        "        return a, da\n",
        "    return a\n",
        "\n",
        "\n",
        "def logistic(z, derivative=False):\n",
        "    a = 1/(1 + np.exp(-z))\n",
        "    if derivative:\n",
        "        da = np.ones(z.shape)\n",
        "        return a, da\n",
        "    return a\n",
        "\n",
        "\n",
        "def softmax(z, derivative=False):\n",
        "    e = np.exp(z - np.max(z, axis=0))\n",
        "    a = e / np.sum(e, axis=0)\n",
        "    if derivative:\n",
        "        da = np.ones(z.shape)\n",
        "        return a, da\n",
        "    return a\n",
        "\n",
        "# Activation functions for hidden layers -----------------------------\n",
        "\n",
        "def tanh(z, derivative=False):\n",
        "    a = np.tanh(z)\n",
        "    if derivative:\n",
        "        da = (1 - a) * (1 + a)\n",
        "        return a, da\n",
        "    return a\n",
        "\n",
        "\n",
        "def relu(z, derivative=False):\n",
        "    a = z * (z >= 0)\n",
        "    if derivative:\n",
        "        da = np.array(z >= 0, dtype=float)\n",
        "        return a, da\n",
        "    return a\n",
        "\n",
        "def logistic_hidden(z, derivative=False):\n",
        "    a = 1/(1 + np.exp(-z))\n",
        "    if derivative:\n",
        "        da = a * (1 - a)\n",
        "        return a, da\n",
        "    return a"
      ],
      "metadata": {
        "id": "jwqrfdjhEHjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Multilayer Perceptron\n",
        "Dr. Carlos Villaseñor\n",
        "\"\"\"\n",
        "\n",
        "class MLP:\n",
        "\n",
        "    def __init__(self, layers_dims,\n",
        "                 hidden_activation=tanh,\n",
        "                 output_activation=logistic):\n",
        "\n",
        "        # Attributes\n",
        "        self.L = len(layers_dims) - 1\n",
        "        self.w = [None] * (self.L + 1)\n",
        "        self.b = [None] * (self.L + 1)\n",
        "        self.f = [None] * (self.L + 1)\n",
        "\n",
        "        # Initialize weights\n",
        "        for l in range(1, self.L + 1):\n",
        "            self.w[l] = -1 + 2 * np.random.rand(layers_dims[l],\n",
        "                                                layers_dims[l-1])\n",
        "            self.b[l] = -1 + 2 * np.random.rand(layers_dims[l], 1)\n",
        "\n",
        "            if l == self.L:\n",
        "                self.f[l] = output_activation\n",
        "            else:\n",
        "                self.f[l] = hidden_activation\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        a = X\n",
        "        for l in range(1, self.L + 1):\n",
        "            z = np.dot(self.w[l], a) + self.b[l]\n",
        "            a = self.f[l](z)\n",
        "        return a\n",
        "\n",
        "    def train(self, X, Y, epochs=500, lr=0.1):\n",
        "        p = X.shape[1]\n",
        "        for _ in range(epochs):\n",
        "            # initialize activations\n",
        "            a = [None] * (self.L + 1)\n",
        "            da = [None] * (self.L + 1)\n",
        "            lg = [None] * (self.L + 1)\n",
        "\n",
        "            # Propagation\n",
        "            a[0] = X\n",
        "            for l in range(1, self.L + 1):\n",
        "                z = np.dot(self.w[l], a[l-1]) + self.b[l]\n",
        "                a[l], da[l] = self.f[l](z, derivative=True)\n",
        "\n",
        "            # Backpropagation\n",
        "            for l in range(self.L, 0, -1):\n",
        "                if l == self.L:\n",
        "                    lg[l] = -(Y - a[l]) * da[l]\n",
        "                else:\n",
        "                    lg[l] = np.dot(self.w[l+1].T, lg[l + 1]) * da[l]\n",
        "\n",
        "            # Gradient Descent\n",
        "            for l in range(1, self.L + 1):\n",
        "                self.w[l] -= (lr/p) * np.dot(lg[l], a[l - 1].T)\n",
        "                self.b[l] -= (lr/p) * np.sum(lg[l])"
      ],
      "metadata": {
        "id": "Ck6BuQz2Ebg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delay block"
      ],
      "metadata": {
        "id": "Xo37Px0_EhPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "DelayBlock class\n",
        "\n",
        "Implementa un buffer de retardos para una señal vectorial\n",
        "\n",
        "Dr. Carlos Villaseñor\n",
        "'''\n",
        "import numpy as np\n",
        "\n",
        "class DelayBlock:\n",
        "\n",
        "    def __init__(self, n_inputs, n_delays):\n",
        "        # Se crea la memoria del buffer como una matriz\n",
        "        self.memory = np.zeros((n_inputs, n_delays))\n",
        "\n",
        "    def add(self, x):\n",
        "        # Creo una copia profunda para evitar conflictos\n",
        "        # con la señal original\n",
        "        x_new = x.copy().reshape(1,-1)\n",
        "\n",
        "        # Implementacion del retardo\n",
        "        self.memory[:,1:] = self.memory[:,:-1]\n",
        "        self.memory[:,0] = x_new\n",
        "\n",
        "    def get(self):\n",
        "        # Retornar la memoria como un solo vector\n",
        "        return self.memory.reshape(-1, 1, order='F')\n",
        "\n",
        "    def add_and_get(self, x):\n",
        "        # Unificación de las dos funcionalidades en una sola\n",
        "        self.add(x)\n",
        "        return self.memory.reshape(-1, 1, order='F')\n",
        "\n",
        "# Test code ----------------------------------------\n",
        "'''\n",
        "A = [np.random.rand(2,1) for i in range(50)]\n",
        "db = db = DelayBlock(2,5)\n",
        "for i in range(6):\n",
        "    db.add(A[i])\n",
        "    print(db.get())\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AAab6apKEg9O",
        "outputId": "6594c19e-2933-491a-98a9-428ef102f799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nA = [np.random.rand(2,1) for i in range(50)]\\ndb = db = DelayBlock(2,5)\\nfor i in range(6):\\n    db.add(A[i])\\n    print(db.get())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NARX"
      ],
      "metadata": {
        "id": "2wAHgUIZEoXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NARX:\n",
        "    '''\n",
        "    Non linear AutoRegressor with eXogeneous inputs.\n",
        "\n",
        "    Con red neuronal densa multicapa como función no lineal.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs, n_delays,\n",
        "                       dense_hidden_layers=(100,),\n",
        "                       learning_rate = 0.01,\n",
        "                       n_repeat_train = 5):\n",
        "        self.net = MLP(((n_inputs+n_outputs)*n_delays,\n",
        "                   *dense_hidden_layers, n_outputs),\n",
        "                     output_activation=linear)\n",
        "        self.dbx = DelayBlock(n_inputs, n_delays)\n",
        "        self.dby = DelayBlock(n_outputs, n_delays)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_repeat_train = n_repeat_train\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predicción NARX.\n",
        "        x es un vector de tamaño (n_inputs, 1)\n",
        "        '''\n",
        "\n",
        "        # Preparar entrada extendida en el tiempo\n",
        "        X_block = self.dbx.add_and_get(x)\n",
        "        Y_est_block = self.dby.get()\n",
        "        net_input = np.vstack((X_block, Y_est_block))\n",
        "\n",
        "\n",
        "        # Predicción de la red neuronal\n",
        "        y_est = self.net.predict(net_input)\n",
        "\n",
        "        # Guardar predicción en el bloque recurrente\n",
        "        self.dby.add(y_est)\n",
        "\n",
        "        # Retornar predicción\n",
        "        return y_est\n",
        "\n",
        "    def predict_and_train(self, x, y):\n",
        "        '''\n",
        "        Predicción y entranemiento en línea de NARX.\n",
        "        x es la entrada un vector de tamaño (n_inputs, 1)\n",
        "        y es la salida deseada un vector de tamaño (n_outputs, 1)\n",
        "\n",
        "        La predicción se entrega antes del entrenamiento pero no se guarda\n",
        "        hasta despues de entrenar.\n",
        "        '''\n",
        "\n",
        "        # Preparar entrada extendida en el tiempo\n",
        "        X_block = self.dbx.add_and_get(x)\n",
        "        Y_est_block = self.dby.get()\n",
        "        net_input = np.vstack((X_block, Y_est_block))\n",
        "\n",
        "        # Predicción de la red neuronal\n",
        "        y_est = self.net.predict(net_input)\n",
        "\n",
        "        # Entrenar red neuronal\n",
        "        self.net.train(net_input, y,\n",
        "                       epochs=self.n_repeat_train,\n",
        "                       lr = self.learning_rate)\n",
        "\n",
        "        # Guardar predicción en el bloque recurrente\n",
        "        self.dby.add(y_est)\n",
        "\n",
        "        # Retornar predicción\n",
        "        return y_est"
      ],
      "metadata": {
        "id": "FzLXRsJrEqn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test code"
      ],
      "metadata": {
        "id": "K-qHTIvLEwZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.integrate import odeint\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def lorenz(u,t):\n",
        "    \"\"\"\n",
        "    Sistema de Lorenz\n",
        "    \"\"\"\n",
        "    s=10; r=28; b=2.667\n",
        "    ux_dot = s*(u[1] - u[0])\n",
        "    uy_dot = r*u[0] - u[1] - u[0]*u[2]\n",
        "    uz_dot = u[0]*u[1] - b*u[2]\n",
        "    return [ux_dot, uy_dot, uz_dot]\n",
        "\n",
        "#Condicion inicial\n",
        "u0 = [0., 1., 1.05]\n",
        "\n",
        "#Tiempo\n",
        "t = np.linspace(0,100,10000)\n",
        "\n",
        "#Soluciones\n",
        "u = odeint(lorenz,u0,t)\n",
        "\n",
        "\n",
        "# Test NARX a un paso\n",
        "narx = NARX(3, 3, 10,\n",
        "            dense_hidden_layers=(100,),\n",
        "            learning_rate=0.01, n_repeat_train=1)\n",
        "y_est = np.zeros((3, 10000))\n",
        "u = u.T\n",
        "for i in range(10000-1):\n",
        "    x = u[:,i].reshape(-1,1)\n",
        "    y = u[:,i+1].reshape(-1,1)\n",
        "    y_est[:,i] = narx.predict_and_train(x, y).ravel()\n",
        "\n",
        "#Gráfica\n",
        "fig = plt.figure()\n",
        "fig.add_subplot(1, 1, 1, projection='3d')\n",
        "#ax = fig.gca(projection='3d')\n",
        "ax = fig.gca()\n",
        "ax.plot(u[0,:], u[1,:], u[2,:], lw=0.7)\n",
        "ax.plot(y_est[0,:], y_est[1,:], y_est[2,:], lw=0.5)\n",
        "ax.set_xlabel(\"Eje X\")\n",
        "ax.set_ylabel(\"Eje Y\")\n",
        "ax.set_zlabel(\"Eje Z\")\n",
        "ax.set_title(\"Atractor de Lorenz\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jJRdJs_NEyp0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}